source: https://bug-100450-attachments.webkit.org/attachment.cgi?id=197712&action=diff&format=raw&headers=1
submitter: Yuqiang Xian
bug report: https://bugs.webkit.org/show_bug.cgi?id=100450

--

--- Source/JavaScriptCore/ChangeLog	(revision 148063)
+++ Source/JavaScriptCore/ChangeLog	(working copy)
@@ -1,3 +1,80 @@ 
+2013-03-21  Yuqiang Xian  <yuqiang.xian@intel.com>
+
+        x32 backend of the JIT compilers
+        https://bugs.webkit.org/show_bug.cgi?id=100450
+
+        Reviewed by NOBODY (OOPS!).
+
+        This patch adds necessary changes to support x32 in the JIT compilers.
+
+        * assembler/AbstractMacroAssembler.h:
+        (TrustedImm32):
+        (Imm32):
+        (TrustedImm64):
+        (Imm64):
+        * assembler/MacroAssembler.h:
+        (MacroAssembler):
+        (JSC::MacroAssembler::comparePtr):
+        (JSC::MacroAssembler::branchPtr):
+        (JSC::MacroAssembler::move):
+        (JSC::MacroAssembler::storePtr):
+        (JSC::MacroAssembler::rotateRightPtr):
+        (JSC::MacroAssembler::shouldBlind):
+        * assembler/MacroAssemblerX86Common.h:
+        (JSC::MacroAssemblerX86Common::push):
+        (JSC::MacroAssemblerX86Common::move):
+        (JSC::MacroAssemblerX86Common::jump):
+        (JSC::MacroAssemblerX86Common::call):
+        * assembler/MacroAssemblerX86_64.h:
+        (MacroAssemblerX86_64):
+        (JSC::MacroAssemblerX86_64::rotateRight32):
+        (JSC::MacroAssemblerX86_64::store32):
+        (JSC::MacroAssemblerX86_64::branch32):
+        (JSC::MacroAssemblerX86_64::convertibleLoadPtr):
+        (JSC::MacroAssemblerX86_64::moveWithPatch):
+        (JSC::MacroAssemblerX86_64::branchPtrWithPatch):
+        (JSC::MacroAssemblerX86_64::storePtrWithPatch):
+        (JSC::MacroAssemblerX86_64::startOfBranchPtrWithPatchOnRegister):
+        (JSC::MacroAssemblerX86_64::revertJumpReplacementToPatchableBranchPtrWithPatch):
+        (JSC::MacroAssemblerX86_64::revertJumpReplacementToBranchPtrWithPatch):
+        * assembler/X86Assembler.h:
+        (JSC::X86Assembler::rorl_i8r):
+        (X86Assembler):
+        (JSC::X86Assembler::testl_rm):
+        (JSC::X86Assembler::movl_mEAX):
+        (JSC::X86Assembler::movl_EAXm):
+        (JSC::X86Assembler::movq_mEAX):
+        (JSC::X86Assembler::movq_EAXm):
+        (JSC::X86Assembler::revertJumpTo_movl_i32r):
+        * dfg/DFGEdge.h:
+        (DFG):
+        (JSC::DFG::Edge::Edge):
+        (Edge):
+        (JSC::DFG::Edge::setNode):
+        (JSC::DFG::Edge::useKindUnchecked):
+        (JSC::DFG::Edge::setUseKind):
+        (JSC::DFG::Edge::setProofStatus):
+        (JSC::DFG::Edge::operator==):
+        * dfg/DFGNode.h:
+        (OpInfo):
+        * dfg/DFGSpeculativeJIT64.cpp:
+        (JSC::DFG::SpeculativeJIT::emitCall):
+        * jit/ExecutableAllocator.h:
+        (JSC):
+        * jit/JITCall.cpp:
+        (JSC::JIT::compileOpCall):
+        (JSC::JIT::compileOpCallSlowCase):
+        (JSC::JIT::privateCompileClosureCall):
+        * jit/JITPropertyAccess.cpp:
+        (JSC::JIT::emit_op_get_scoped_var):
+        (JSC::JIT::emit_op_put_scoped_var):
+        * jit/JITStubs.cpp:
+        * jit/JITStubs.h:
+        (JITStackFrame):
+        (JSC::JITStackFrame::returnAddressSlot):
+        * runtime/MatchResult.h:
+        (MatchResult):
+
 2013-04-09  Oliver Hunt  <oliver@apple.com>
 
         Add liveness tests to JSC API entry points
--- Source/JavaScriptCore/assembler/AbstractMacroAssembler.h	(revision 148063)
+++ Source/JavaScriptCore/assembler/AbstractMacroAssembler.h	(working copy)
@@ -229,7 +229,7 @@ public:
         {
         }
 
-#if !CPU(X86_64)
+#if !CPU(X86_64) || CPU(X32)
         explicit TrustedImm32(TrustedImmPtr ptr)
             : m_value(ptr.asIntptr())
         {
@@ -251,7 +251,7 @@ public:
             : TrustedImm32(value)
         {
         }
-#if !CPU(X86_64)
+#if !CPU(X86_64) || CPU(X32)
         explicit Imm32(TrustedImmPtr ptr)
             : TrustedImm32(ptr)
         {
@@ -275,7 +275,7 @@ public:
         {
         }
 
-#if CPU(X86_64)
+#if CPU(X86_64) && !CPU(X32)
         explicit TrustedImm64(TrustedImmPtr ptr)
             : m_value(ptr.asIntptr())
         {
@@ -296,7 +296,7 @@ public:
             : TrustedImm64(value)
         {
         }
-#if CPU(X86_64)
+#if CPU(X86_64) && !CPU(X32)
         explicit Imm64(TrustedImmPtr ptr)
             : TrustedImm64(ptr)
         {
--- Source/JavaScriptCore/assembler/MacroAssembler.h	(revision 148063)
+++ Source/JavaScriptCore/assembler/MacroAssembler.h	(working copy)
@@ -325,7 +325,7 @@ public:
     // Ptr methods
     // On 32-bit platforms (i.e. x86), these methods directly map onto their 32-bit equivalents.
     // FIXME: should this use a test for 32-bitness instead of this specific exception?
-#if !CPU(X86_64)
+#if !CPU(X86_64) || CPU(X32)
     void addPtr(Address src, RegisterID dest)
     {
         add32(src, dest);
@@ -447,16 +447,16 @@ public:
         return load32WithCompactAddressOffsetPatch(address, dest);
     }
 
-    void move(ImmPtr imm, RegisterID dest)
+    void comparePtr(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID dest)
     {
-        move(Imm32(imm.asTrustedImmPtr()), dest);
+        compare32(cond, left, right, dest);
     }
 
-    void comparePtr(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID dest)
+    void comparePtr(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID dest)
     {
         compare32(cond, left, right, dest);
     }
-
+    
     void storePtr(RegisterID src, ImplicitAddress address)
     {
         store32(src, address);
@@ -477,11 +477,6 @@ public:
         store32(TrustedImm32(imm), address);
     }
     
-    void storePtr(ImmPtr imm, Address address)
-    {
-        store32(Imm32(imm.asTrustedImmPtr()), address);
-    }
-
     void storePtr(TrustedImmPtr imm, void* address)
     {
         store32(TrustedImm32(imm), address);
@@ -501,11 +496,6 @@ public:
     {
         return branch32(cond, left, TrustedImm32(right));
     }
-    
-    Jump branchPtr(RelationalCondition cond, RegisterID left, ImmPtr right)
-    {
-        return branch32(cond, left, Imm32(right.asTrustedImmPtr()));
-    }
 
     Jump branchPtr(RelationalCondition cond, RegisterID left, Address right)
     {
@@ -571,6 +561,28 @@ public:
     {
         return MacroAssemblerBase::branchTest8(cond, Address(address.base, address.offset), mask);
     }
+
+#if !CPU(X86_64)
+    void move(ImmPtr imm, RegisterID dest)
+    {
+        move(Imm32(imm.asTrustedImmPtr()), dest);
+    }
+
+    void storePtr(ImmPtr imm, Address address)
+    {
+        store32(Imm32(imm.asTrustedImmPtr()), address);
+    }
+
+    Jump branchPtr(RelationalCondition cond, RegisterID left, ImmPtr right)
+    {
+        return branch32(cond, left, Imm32(right.asTrustedImmPtr()));
+    }
+#else
+    void rotateRightPtr(TrustedImm32 imm, RegisterID srcDst)
+    {
+        rotateRight32(imm, srcDst);
+    }
+#endif
 #else
     void addPtr(RegisterID src, RegisterID dest)
     {
@@ -846,6 +858,9 @@ public:
     {
         return branchSub64(cond, src1, src2, dest);
     }
+#endif // !CPU(X86_64) || CPU(X32)
+
+#if CPU(X86_64)
 
 #if ENABLE(JIT_CONSTANT_BLINDING)
     using MacroAssemblerBase::and64;
@@ -890,10 +905,12 @@ public:
         case 0xffff:
         case 0xffffff:
         case 0xffffffffL:
+#if !CPU(X32)
         case 0xffffffffffL:
         case 0xffffffffffffL:
         case 0xffffffffffffffL:
         case 0xffffffffffffffffL:
+#endif
             return false;
         default: {
             if (value <= 0xff)
@@ -1069,7 +1086,7 @@ public:
 
 #endif
 
-#endif // !CPU(X86_64)
+#endif // CPU(X86_64)
 
 #if ENABLE(JIT_CONSTANT_BLINDING)
     bool shouldBlind(Imm32 imm)
--- Source/JavaScriptCore/assembler/MacroAssemblerX86Common.h	(revision 148063)
+++ Source/JavaScriptCore/assembler/MacroAssemblerX86Common.h	(working copy)
@@ -963,7 +963,15 @@ public:
 
     void push(Address address)
     {
+#if CPU(X32)
+        // On X86_64 (also in X32 mode), the default operand size for call/jump/push/pop
+        // is 64-bit. This means if we use a memory operand for those instructions,
+        // we'll be loading a 64-bit data from the memory address. It's not our intention.
+        load32(address, scratchRegister);
+        m_assembler.push_r(scratchRegister);
+#else
         m_assembler.push_m(address.offset, address.base);
+#endif
     }
 
     void push(TrustedImm32 imm)
@@ -997,7 +1005,11 @@ public:
 
     void move(TrustedImmPtr imm, RegisterID dest)
     {
+#if CPU(X32)
+        m_assembler.movl_i32r(imm.asIntptr(), dest);
+#else
         m_assembler.movq_i64r(imm.asIntptr(), dest);
+#endif
     }
 
     void move(TrustedImm64 imm, RegisterID dest)
@@ -1196,7 +1208,15 @@ public:
     // Address is a memory location containing the address to jump to
     void jump(Address address)
     {
+#if CPU(X32)
+        // On X86_64 (also in X32 mode), the default operand size for call/jump/push/pop
+        // is 64-bit. This means if we use a memory operand for those instructions,
+        // we'll be loading a 64-bit data from the memory address. It's not our intention.
+        load32(address, scratchRegister);
+        m_assembler.jmp_r(scratchRegister);
+#else
         m_assembler.jmp_m(address.offset, address.base);
+#endif
     }
 
 
@@ -1363,7 +1383,15 @@ public:
 
     void call(Address address)
     {
+#if CPU(X32)
+        // On X86_64 (also in X32 mode), the default operand size for call/jump/push/pop
+        // is 64-bit. This means if we use a memory operand for those instructions,
+        // we'll be loading a 64-bit data from the memory address. It's not our intention.
+        load32(address, scratchRegister);
+        m_assembler.call(scratchRegister);
+#else
         m_assembler.call_m(address.offset, address.base);
+#endif
     }
 
     void ret()
--- Source/JavaScriptCore/assembler/MacroAssemblerX86_64.h	(revision 148063)
+++ Source/JavaScriptCore/assembler/MacroAssemblerX86_64.h	(working copy)
@@ -36,10 +36,15 @@ namespace JSC {
 
 class MacroAssemblerX86_64 : public MacroAssemblerX86Common {
 public:
+#if CPU(X32)
+    static const Scale ScalePtr = TimesFour;
+#else
     static const Scale ScalePtr = TimesEight;
+#endif
 
     using MacroAssemblerX86Common::add32;
     using MacroAssemblerX86Common::and32;
+    using MacroAssemblerX86Common::branch32;
     using MacroAssemblerX86Common::branchAdd32;
     using MacroAssemblerX86Common::or32;
     using MacroAssemblerX86Common::sub32;
@@ -110,12 +115,39 @@ public:
         m_assembler.cvtsi2sd_rr(scratchRegister, dest);
     }
 
+    void rotateRight32(TrustedImm32 imm, RegisterID srcDst)
+    {
+        m_assembler.rorl_i8r(imm.m_value, srcDst);
+    }
+
     void store32(TrustedImm32 imm, void* address)
     {
         move(TrustedImmPtr(address), scratchRegister);
         store32(imm, scratchRegister);
     }
     
+    void store32(RegisterID src, void* address)
+    {
+        if (src == X86Registers::eax)
+            m_assembler.movl_EAXm(address);
+        else {
+            move(TrustedImmPtr(address), scratchRegister);
+            store32(src, scratchRegister);
+        }
+    }
+    
+    Jump branch32(RelationalCondition cond, AbsoluteAddress left, RegisterID right)
+    {
+        move(TrustedImmPtr(left.m_ptr), scratchRegister);
+        return branch32(cond, Address(scratchRegister), right);
+    }
+
+    Jump branch32(RelationalCondition cond, AbsoluteAddress left, TrustedImm32 right)
+    {
+        move(TrustedImmPtr(left.m_ptr), scratchRegister);
+        return branch32(cond, left, right);
+    }
+
     void store8(TrustedImm32 imm, void* address)
     {
         move(TrustedImmPtr(address), scratchRegister);
@@ -528,33 +560,53 @@ public:
     ConvertibleLoadLabel convertibleLoadPtr(Address address, RegisterID dest)
     {
         ConvertibleLoadLabel result = ConvertibleLoadLabel(this);
+#if CPU(X32)
+        m_assembler.movl_mr(address.offset, address.base, dest);
+#else
         m_assembler.movq_mr(address.offset, address.base, dest);
+#endif
         return result;
     }
 
     DataLabelPtr moveWithPatch(TrustedImmPtr initialValue, RegisterID dest)
     {
         padBeforePatch();
+#if CPU(X32)
+        m_assembler.movl_i32r(initialValue.asIntptr(), dest);
+#else
         m_assembler.movq_i64r(initialValue.asIntptr(), dest);
+#endif
         return DataLabelPtr(this);
     }
 
     Jump branchPtrWithPatch(RelationalCondition cond, RegisterID left, DataLabelPtr& dataLabel, TrustedImmPtr initialRightValue = TrustedImmPtr(0))
     {
         dataLabel = moveWithPatch(initialRightValue, scratchRegister);
+#if CPU(X32)
+        return branch32(cond, left, scratchRegister);
+#else
         return branch64(cond, left, scratchRegister);
+#endif
     }
 
     Jump branchPtrWithPatch(RelationalCondition cond, Address left, DataLabelPtr& dataLabel, TrustedImmPtr initialRightValue = TrustedImmPtr(0))
     {
         dataLabel = moveWithPatch(initialRightValue, scratchRegister);
+#if CPU(X32)
+        return branch32(cond, left, scratchRegister);
+#else
         return branch64(cond, left, scratchRegister);
+#endif
     }
 
     DataLabelPtr storePtrWithPatch(TrustedImmPtr initialValue, ImplicitAddress address)
     {
         DataLabelPtr label = moveWithPatch(initialValue, scratchRegister);
+#if CPU(X32)
+        store32(scratchRegister, address);
+#else
         store64(scratchRegister, address);
+#endif
         return label;
     }
     
@@ -591,7 +643,7 @@ public:
     {
         const int rexBytes = 1;
         const int opcodeBytes = 1;
-        const int immediateBytes = 8;
+        const int immediateBytes = sizeof(void*);
         const int totalBytes = rexBytes + opcodeBytes + immediateBytes;
         ASSERT(totalBytes >= maxJumpReplacementSize());
         return label.labelAtOffset(-totalBytes);
@@ -604,12 +656,20 @@ public:
     
     static void revertJumpReplacementToPatchableBranchPtrWithPatch(CodeLocationLabel instructionStart, Address, void* initialValue)
     {
-        X86Assembler::revertJumpTo_movq_i64r(instructionStart.executableAddress(), reinterpret_cast<intptr_t>(initialValue), scratchRegister);
+#if CPU(X32)
+        X86Assembler::revertJumpTo_movl_i32r(instructionStart.executableAddress(), reinterpret_cast<uintptr_t>(initialValue), scratchRegister);
+#else
+        X86Assembler::revertJumpTo_movq_i64r(instructionStart.executableAddress(), reinterpret_cast<uintptr_t>(initialValue), scratchRegister);
+#endif
     }
 
     static void revertJumpReplacementToBranchPtrWithPatch(CodeLocationLabel instructionStart, RegisterID, void* initialValue)
     {
-        X86Assembler::revertJumpTo_movq_i64r(instructionStart.executableAddress(), reinterpret_cast<intptr_t>(initialValue), scratchRegister);
+#if CPU(X32)
+        X86Assembler::revertJumpTo_movl_i32r(instructionStart.executableAddress(), reinterpret_cast<uintptr_t>(initialValue), scratchRegister);
+#else
+        X86Assembler::revertJumpTo_movq_i64r(instructionStart.executableAddress(), reinterpret_cast<uintptr_t>(initialValue), scratchRegister);
+#endif
     }
 
 private:
--- Source/JavaScriptCore/assembler/X86Assembler.h	(revision 148063)
+++ Source/JavaScriptCore/assembler/X86Assembler.h	(working copy)
@@ -683,6 +683,16 @@ public:
         }
     }
 
+    void rorl_i8r(int imm, RegisterID dst)
+    {
+        if (imm == 1)
+            m_formatter.oneByteOp(OP_GROUP2_Ev1, GROUP2_OP_ROR, dst);
+        else {
+            m_formatter.oneByteOp(OP_GROUP2_EvIb, GROUP2_OP_ROR, dst);
+            m_formatter.immediate8(imm);
+        }
+    }
+
 #endif
 
     void sarl_i8r(int imm, RegisterID dst)
@@ -1007,6 +1017,11 @@ public:
         m_formatter.oneByteOp64(OP_TEST_EvGv, src, base, offset);
     }
 
+    void testl_rm(RegisterID src, int offset, RegisterID base)
+    {
+        m_formatter.oneByteOp(OP_TEST_EvGv, src, base, offset);
+    }
+
     void testq_i32r(int imm, RegisterID dst)
     {
         m_formatter.oneByteOp64(OP_GROUP3_EvIz, GROUP3_OP_TEST, dst);
@@ -1111,7 +1126,8 @@ public:
     {
         m_formatter.oneByteOp(OP_MOV_EAXOv);
 #if CPU(X86_64)
-        m_formatter.immediate64(reinterpret_cast<int64_t>(addr));
+        // Make sure the addr is zero extended (if it's a 32-bit addr in X32 mode).
+        m_formatter.immediate64(reinterpret_cast<uintptr_t>(addr));
 #else
         m_formatter.immediate32(reinterpret_cast<int>(addr));
 #endif
@@ -1193,7 +1209,8 @@ public:
     {
         m_formatter.oneByteOp(OP_MOV_OvEAX);
 #if CPU(X86_64)
-        m_formatter.immediate64(reinterpret_cast<int64_t>(addr));
+        // Make sure the addr is zero extended (if it's a 32-bit addr in X32 mode).
+        m_formatter.immediate64(reinterpret_cast<uintptr_t>(addr));
 #else
         m_formatter.immediate32(reinterpret_cast<int>(addr));
 #endif
@@ -1223,13 +1240,15 @@ public:
     void movq_mEAX(const void* addr)
     {
         m_formatter.oneByteOp64(OP_MOV_EAXOv);
-        m_formatter.immediate64(reinterpret_cast<int64_t>(addr));
+        // Make sure the addr is zero extended (if it's a 32-bit addr in X32 mode).
+        m_formatter.immediate64(reinterpret_cast<uintptr_t>(addr));
     }
 
     void movq_EAXm(const void* addr)
     {
         m_formatter.oneByteOp64(OP_MOV_OvEAX);
-        m_formatter.immediate64(reinterpret_cast<int64_t>(addr));
+        // Make sure the addr is zero extended (if it's a 32-bit addr in X32 mode).
+        m_formatter.immediate64(reinterpret_cast<uintptr_t>(addr));
     }
 
     void movq_mr(int offset, RegisterID base, RegisterID dst)
@@ -1899,6 +1918,25 @@ public:
         } u;
         u.asWord = imm;
         for (unsigned i = rexBytes + opcodeBytes; i < static_cast<unsigned>(maxJumpReplacementSize()); ++i)
+            ptr[i] = u.asBytes[i - rexBytes - opcodeBytes];
+    }
+
+    static void revertJumpTo_movl_i32r(void* instructionStart, int32_t imm, RegisterID dst)
+    {
+        const int rexBytes = (dst >= X86Registers::r8) ? 1 : 0;
+        const int opcodeBytes = 1;
+        ASSERT(rexBytes + opcodeBytes <= maxJumpReplacementSize());
+        uint8_t* ptr = reinterpret_cast<uint8_t*>(instructionStart);
+        if (dst >= X86Registers::r8)
+            ptr[0] = PRE_REX | (dst >> 3);
+        ptr[rexBytes] = OP_MOV_EAXIv | (dst & 7);
+        
+        union {
+            uint32_t asWord;
+            uint8_t asBytes[4];
+        } u;
+        u.asWord = imm;
+        for (unsigned i = rexBytes + opcodeBytes; i < static_cast<unsigned>(maxJumpReplacementSize()); ++i)
             ptr[i] = u.asBytes[i - rexBytes - opcodeBytes];
     }
 #endif
--- Source/JavaScriptCore/dfg/DFGEdge.h	(revision 148063)
+++ Source/JavaScriptCore/dfg/DFGEdge.h	(working copy)
@@ -35,12 +35,18 @@ 
 
 namespace JSC { namespace DFG {
 
+#if USE(JSVALUE64) && !CPU(X32)
+#define ENCODE_IN_NODE_POINTER 1
+#else
+#define ENCODE_IN_NODE_POINTER 0
+#endif
+
 class AdjacencyList;
 
 class Edge {
 public:
     explicit Edge(Node* node = 0, UseKind useKind = UntypedUse, ProofStatus proofStatus = NeedsCheck)
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
         : m_encodedWord(makeWord(node, useKind, proofStatus))
 #else
         : m_node(node)
@@ -49,7 +55,7 @@ public:
     {
     }
     
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
     Node* node() const { return bitwise_cast<Node*>(m_encodedWord >> shift()); }
 #else
     Node* node() const { return m_node; }
@@ -60,7 +66,7 @@ public:
     
     void setNode(Node* node)
     {
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
         m_encodedWord = makeWord(node, useKind(), proofStatus());
 #else
         m_node = node;
@@ -69,7 +75,7 @@ public:
     
     UseKind useKindUnchecked() const
     {
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
         unsigned masked = m_encodedWord & (((1 << shift()) - 1));
         unsigned shifted = masked >> 1;
 #else
@@ -88,7 +94,7 @@ public:
     void setUseKind(UseKind useKind)
     {
         ASSERT(node());
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
         m_encodedWord = makeWord(node(), useKind, proofStatus());
 #else
         m_encodedWord = makeWord(useKind, proofStatus());
@@ -107,7 +113,7 @@ public:
     void setProofStatus(ProofStatus proofStatus)
     {
         ASSERT(node());
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
         m_encodedWord = makeWord(node(), useKind(), proofStatus);
 #else
         m_encodedWord = makeWord(useKind(), proofStatus);
@@ -131,7 +137,7 @@ public:
     
     bool operator==(Edge other) const
     {
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
         return m_encodedWord == other.m_encodedWord;
 #else
         return m_node == other.m_node && m_encodedWord == other.m_encodedWord;
@@ -147,7 +153,7 @@ public:
 private:
     friend class AdjacencyList;
     
-#if USE(JSVALUE64)
+#if ENCODE_IN_NODE_POINTER
     static uint32_t shift() { return 6; }
     
     static uintptr_t makeWord(Node* node, UseKind useKind, ProofStatus proofStatus)
--- Source/JavaScriptCore/dfg/DFGNode.h	(revision 148063)
+++ Source/JavaScriptCore/dfg/DFGNode.h	(working copy)
@@ -72,7 +72,7 @@ struct NewArrayBufferData {
 struct OpInfo {
     explicit OpInfo(int32_t value) : m_value(static_cast<uintptr_t>(value)) { }
     explicit OpInfo(uint32_t value) : m_value(static_cast<uintptr_t>(value)) { }
-#if OS(DARWIN) || USE(JSVALUE64)
+#if OS(DARWIN) || (CPU(X86_64) && !CPU(X32))
     explicit OpInfo(size_t value) : m_value(static_cast<uintptr_t>(value)) { }
 #endif
     explicit OpInfo(void* value) : m_value(reinterpret_cast<uintptr_t>(value)) { }
--- Source/JavaScriptCore/dfg/DFGSpeculativeJIT64.cpp	(revision 148063)
+++ Source/JavaScriptCore/dfg/DFGSpeculativeJIT64.cpp	(working copy)
@@ -785,6 +785,9 @@ void SpeculativeJIT::emitCall(Node* node
     m_jit.addPtr(TrustedImm32(m_jit.codeBlock()->m_numCalleeRegisters * sizeof(Register)), GPRInfo::callFrameRegister);
     
     slowPath.append(m_jit.branchPtrWithPatch(MacroAssembler::NotEqual, calleeGPR, targetToCheck, MacroAssembler::TrustedImmPtr(0)));
+#if CPU(X32)
+    slowPath.append(m_jit.branchTest64(MacroAssembler::NonZero, calleeGPR, GPRInfo::tagMaskRegister));
+#endif
 
     m_jit.loadPtr(MacroAssembler::Address(calleeGPR, OBJECT_OFFSETOF(JSFunction, m_scope)), resultGPR);
     m_jit.store64(resultGPR, MacroAssembler::Address(GPRInfo::callFrameRegister, static_cast<ptrdiff_t>(sizeof(Register)) * JSStack::ScopeChain));
--- Source/JavaScriptCore/heap/Region.h	(revision 148063)
+++ Source/JavaScriptCore/heap/Region.h	(working copy)
@@ -35,7 +35,7 @@ 
 #define HEAP_MEMORY_ID reinterpret_cast<void*>(static_cast<intptr_t>(-3))
 
 #ifndef ENABLE_SUPER_REGION
-#if USE(JSVALUE64)
+#if USE(JSVALUE64) && !CPU(X32)
 #define ENABLE_SUPER_REGION 1
 #else
 #define ENABLE_SUPER_REGION 0
--- Source/JavaScriptCore/jit/ExecutableAllocator.h	(revision 148063)
+++ Source/JavaScriptCore/jit/ExecutableAllocator.h	(working copy)
@@ -105,7 +105,7 @@ class DemandExecutableAllocator;
 #if ENABLE(EXECUTABLE_ALLOCATOR_FIXED)
 #if CPU(ARM)
 static const size_t fixedExecutableMemoryPoolSize = 16 * 1024 * 1024;
-#elif CPU(X86_64)
+#elif CPU(X86_64) && !CPU(X32)
 static const size_t fixedExecutableMemoryPoolSize = 1024 * 1024 * 1024;
 #else
 static const size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;
--- Source/JavaScriptCore/jit/JITCall.cpp	(revision 148063)
+++ Source/JavaScriptCore/jit/JITCall.cpp	(working copy)
@@ -198,6 +198,11 @@ void JIT::compileOpCall(OpcodeID opcodeI
     END_UNINTERRUPTED_SEQUENCE(sequenceOpCall);
     addSlowCase(slowCase);
 
+#if CPU(X32)
+    // The above branchPtr doesn't catch the case where the callee isn't a cell.
+    addSlowCase(branchTest64(NonZero, regT0, tagMaskRegister));
+#endif
+
     ASSERT(m_callStructureStubCompilationInfo.size() == callLinkInfoIndex);
     m_callStructureStubCompilationInfo.append(StructureStubCompilationInfo());
     m_callStructureStubCompilationInfo[callLinkInfoIndex].hotPathBegin = addressOfLinkedFunctionCheck;
@@ -219,6 +224,9 @@ void JIT::compileOpCallSlowCase(OpcodeID
     }
 
     linkSlowCase(iter);
+#if CPU(X32)
+    linkSlowCase(iter);
+#endif
     
     m_callStructureStubCompilationInfo[callLinkInfoIndex].callReturnLocation = emitNakedCall(opcodeID == op_construct ? m_globalData->getCTIStub(linkConstructGenerator).code() : m_globalData->getCTIStub(linkCallGenerator).code());
 
@@ -229,7 +237,7 @@ void JIT::privateCompileClosureCall(Call
 {
     JumpList slowCases;
     
-    slowCases.append(branchTestPtr(NonZero, regT0, tagMaskRegister));
+    slowCases.append(branchTest64(NonZero, regT0, tagMaskRegister));
     slowCases.append(branchPtr(NotEqual, Address(regT0, JSCell::structureOffset()), TrustedImmPtr(expectedStructure)));
     slowCases.append(branchPtr(NotEqual, Address(regT0, JSFunction::offsetOfExecutable()), TrustedImmPtr(expectedExecutable)));
     
--- Source/JavaScriptCore/jit/JITPropertyAccess.cpp	(revision 148063)
+++ Source/JavaScriptCore/jit/JITPropertyAccess.cpp	(working copy)
@@ -1214,7 +1214,7 @@ void JIT::emit_op_get_scoped_var(Instruc
     if (checkTopLevel && skip--) {
         Jump activationNotCreated;
         if (checkTopLevel)
-            activationNotCreated = branchTestPtr(Zero, addressFor(m_codeBlock->activationRegister()));
+            activationNotCreated = branchTest64(Zero, addressFor(m_codeBlock->activationRegister()));
         loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
         activationNotCreated.link(this);
     }
@@ -1222,7 +1222,7 @@ void JIT::emit_op_get_scoped_var(Instruc
         loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 
     loadPtr(Address(regT0, JSVariableObject::offsetOfRegisters()), regT0);
-    loadPtr(Address(regT0, currentInstruction[2].u.operand * sizeof(Register)), regT0);
+    load64(Address(regT0, currentInstruction[2].u.operand * sizeof(Register)), regT0);
     emitValueProfilingSite();
     emitPutVirtualRegister(currentInstruction[1].u.operand);
 }
@@ -1239,7 +1239,7 @@ void JIT::emit_op_put_scoped_var(Instruc
     if (checkTopLevel && skip--) {
         Jump activationNotCreated;
         if (checkTopLevel)
-            activationNotCreated = branchTestPtr(Zero, addressFor(m_codeBlock->activationRegister()));
+            activationNotCreated = branchTest64(Zero, addressFor(m_codeBlock->activationRegister()));
         loadPtr(Address(regT1, JSScope::offsetOfNext()), regT1);
         activationNotCreated.link(this);
     }
@@ -1249,7 +1249,7 @@ void JIT::emit_op_put_scoped_var(Instruc
     emitWriteBarrier(regT1, regT0, regT2, regT3, ShouldFilterImmediates, WriteBarrierForVariableAccess);
 
     loadPtr(Address(regT1, JSVariableObject::offsetOfRegisters()), regT1);
-    storePtr(regT0, Address(regT1, currentInstruction[1].u.operand * sizeof(Register)));
+    store64(regT0, Address(regT1, currentInstruction[1].u.operand * sizeof(Register)));
 }
 
 void JIT::emit_op_init_global_const(Instruction* currentInstruction)
--- Source/JavaScriptCore/jit/JITStubs.cpp	(revision 148063)
+++ Source/JavaScriptCore/jit/JITStubs.cpp	(working copy)
@@ -370,6 +370,7 @@ SYMBOL_STRING(ctiOpThrowNotCaught) ":" "
 
 #if COMPILER(GCC) && CPU(X86_64)
 
+#if !CPU(X32)
 // These ASSERTs remind you that, if you change the layout of JITStackFrame, you
 // need to change the assembly trampolines below to match.
 COMPILE_ASSERT(offsetof(struct JITStackFrame, callFrame) == 0x58, JITStackFrame_callFrame_offset_matches_ctiTrampoline);
@@ -436,6 +437,72 @@ SYMBOL_STRING(ctiOpThrowNotCaught) ":" "
     "popq %rbp" "\n"
     "ret" "\n"
 );
+#else
+COMPILE_ASSERT(offsetof(struct JITStackFrame, callFrame) == 0x48, JITStackFrame_callFrame_offset_matches_ctiTrampoline);
+COMPILE_ASSERT(offsetof(struct JITStackFrame, code) == 0x40, JITStackFrame_code_offset_matches_ctiTrampoline);
+COMPILE_ASSERT(offsetof(struct JITStackFrame, savedRBX) == 0x58, JITStackFrame_stub_argument_space_matches_ctiTrampoline);
+
+asm (
+".text\n"
+".globl " SYMBOL_STRING(ctiTrampoline) "\n"
+HIDE_SYMBOL(ctiTrampoline) "\n"
+SYMBOL_STRING(ctiTrampoline) ":" "\n"
+    "pushq %rbp" "\n"
+    "movl %esp, %ebp" "\n"
+    "pushq %r12" "\n"
+    "pushq %r13" "\n"
+    "pushq %r14" "\n"
+    "pushq %r15" "\n"
+    "pushq %rbx" "\n"
+    // Form the JIT stubs area
+    "subl $0x58, %esp" "\n"
+    "movl %r9d, 0x54(%esp)" "\n"
+    "movl %r8d, 0x50(%esp)" "\n"
+    "movl %ecx, 0x4c(%esp)" "\n"
+    "movl %edx, 0x48(%esp)" "\n"
+    "movl %esi, 0x44(%esp)" "\n"
+    "movl %edi, 0x40(%esp)" "\n"
+    "movq $512, %r12" "\n"
+    "movq $0xFFFF000000000000, %r14" "\n"
+    "movq $0xFFFF000000000002, %r15" "\n"
+    "movl %edx, %r13d" "\n"
+    "call *%rdi" "\n"
+    "addl $0x58, %esp" "\n"
+    "popq %rbx" "\n"
+    "popq %r15" "\n"
+    "popq %r14" "\n"
+    "popq %r13" "\n"
+    "popq %r12" "\n"
+    "popq %rbp" "\n"
+    "ret" "\n"
+".globl " SYMBOL_STRING(ctiTrampolineEnd) "\n"
+HIDE_SYMBOL(ctiTrampolineEnd) "\n"
+SYMBOL_STRING(ctiTrampolineEnd) ":" "\n"
+);
+
+asm (
+".globl " SYMBOL_STRING(ctiVMThrowTrampoline) "\n"
+HIDE_SYMBOL(ctiVMThrowTrampoline) "\n"
+SYMBOL_STRING(ctiVMThrowTrampoline) ":" "\n"
+    "movl %esp, %edi" "\n"
+    "call " LOCAL_REFERENCE(cti_vm_throw) "\n"
+    "int3" "\n"
+);
+
+asm (
+".globl " SYMBOL_STRING(ctiOpThrowNotCaught) "\n"
+HIDE_SYMBOL(ctiOpThrowNotCaught) "\n"
+SYMBOL_STRING(ctiOpThrowNotCaught) ":" "\n"
+    "addl $0x58, %esp" "\n"
+    "popq %rbx" "\n"
+    "popq %r15" "\n"
+    "popq %r14" "\n"
+    "popq %r13" "\n"
+    "popq %r12" "\n"
+    "popq %rbp" "\n"
+    "ret" "\n"
+);
+#endif
 
 #elif COMPILER(MSVC) && CPU(X86_64)
 
--- Source/JavaScriptCore/jit/JITStubs.h	(revision 148063)
+++ Source/JavaScriptCore/jit/JITStubs.h	(working copy)
@@ -89,9 +89,9 @@ union JITStubArg {
     
 #if !OS(WINDOWS) && CPU(X86_64)
 struct JITStackFrame {
-    void* reserved; // Unused
+    int64_t reserved; // Unused
     JITStubArg args[6];
-    void* padding[2]; // Maintain 32-byte stack alignment (possibly overkill).
+    void* padding[2]; // Maintain 16-byte stack alignment.
 
     void* code;
     JSStack* stack;
@@ -100,16 +100,21 @@ struct JITStackFrame {
     void* unused2;
     JSGlobalData* globalData;
 
-    void* savedRBX;
-    void* savedR15;
-    void* savedR14;
-    void* savedR13;
-    void* savedR12;
-    void* savedRBP;
-    void* savedRIP;
+    int64_t savedRBX;
+    int64_t savedR15;
+    int64_t savedR14;
+    int64_t savedR13;
+    int64_t savedR12;
+    int64_t savedRBP;
+    int64_t savedRIP;
 
     // When JIT code makes a call, it pushes its return address just below the rest of the stack.
+#if CPU(X32)
+    // On X32 the return address occupies a 8-byte slot (with the higher 4 bytes zero'ed)
+    ReturnAddressPtr* returnAddressSlot() { return reinterpret_cast<ReturnAddressPtr*>(this) - 2; }
+#else
     ReturnAddressPtr* returnAddressSlot() { return reinterpret_cast<ReturnAddressPtr*>(this) - 1; }
+#endif
 };
 #elif OS(WINDOWS) && CPU(X86_64)
 struct JITStackFrame {
--- Source/JavaScriptCore/runtime/MatchResult.h	(revision 148063)
+++ Source/JavaScriptCore/runtime/MatchResult.h	(working copy)
@@ -64,8 +64,17 @@ struct MatchResult {
         return start == end;
     }
 
+#if CPU(X32)
+    // On X32, size_t is 32-bit while the register size is 64-bit.
+    // Make each member to be 64-bit so that they will be placed in two registers
+    // when a function (a Yarr JIT function) returns a MatchResult structure,
+    // a behavior consistent with any other architecture.
+    uint64_t start;
+    uint64_t end;
+#else
     size_t start;
     size_t end;
+#endif
 };
 
 #endif
--- Source/WTF/ChangeLog	(revision 148063)
+++ Source/WTF/ChangeLog	(working copy)
@@ -1,3 +1,14 @@ 
+2013-03-21  Yuqiang Xian  <yuqiang.xian@intel.com>
+
+        x32 backend of the JIT compilers
+        https://bugs.webkit.org/show_bug.cgi?id=100450
+
+        Reviewed by NOBODY (OOPS!).
+
+        This patch adds necessary changes to support x32 in the JIT compilers.
+
+        * wtf/Platform.h: Add the definition of CPU(X32)
+
 2013-04-09  Patrick Gansterer  <paroga@webkit.org>
 
         [CMake] Remove conditional source file lists in WTF
--- Source/WTF/wtf/Platform.h	(revision 148063)
+++ Source/WTF/wtf/Platform.h	(working copy)
@@ -160,6 +160,9 @@ 
 #if   defined(__x86_64__) \
     || defined(_M_X64)
 #define WTF_CPU_X86_64 1
+#if defined(__ILP32__)
+#define WTF_CPU_X32 1
+#endif
 #endif
 
 /* CPU(ARM) - ARM, any version*/


